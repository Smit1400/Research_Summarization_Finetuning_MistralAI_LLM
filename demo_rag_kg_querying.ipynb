{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.chains import GraphCypherQAChain, RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import CTransformers, HuggingFaceHub\n",
    "from dotenv import load_dotenv\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all the env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"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\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# os.environ[\"NEO4J_URI\"] = os.getenv(\"NEO4J_URI\")\n",
    "# os.environ[\"NEO4J_USERNAME\"] = os.getenv(\"NEO4J_USERNAME\")\n",
    "# os.environ[\"NEO4J_PASSWORD\"] = os.getenv(\"NEO4J_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We load the Neo4j knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(\n",
    "    url=os.environ[\"NEO4J_URI\"],\n",
    "    username=os.environ[\"NEO4J_USERNAME\"],\n",
    "    password=os.environ[\"NEO4J_PASSWORD\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we load our documents (we only work with on pdf for now, but you can pass as much as you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ingestion():\n",
    "    loader = DirectoryLoader(\"data/\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    load_data = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "    documents = text_splitter.split_documents(load_data)\n",
    "    return documents\n",
    "\n",
    "documents = data_ingestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import PeftModel, PeftConfig\n",
    "# from transformers import AutoModelForCausalLM\n",
    "\n",
    "# config = PeftConfig.from_pretrained(\"smit0104/research_summarization-mistral\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "# llm = PeftModel.from_pretrained(model, \"smit0104/research_summarization-mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we make a langchain pipeline that does the following:\n",
    "\n",
    "1. Load the embeddings\n",
    "2. Load our vectordatabase\n",
    "3. Initialize our LLM\n",
    "4. Prompt engineering\n",
    "5. Bind everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = \"\"\"Use the following pieces of information from the research paper to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt():\n",
    "    \"\"\"\n",
    "    Prompt template for QA retrieval for each vectorstore\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "def retrieval_qa_chain(llm, prompt, db, memory):\n",
    "    \n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(llm=llm, \n",
    "                                                     memory=memory, \n",
    "                                                     retriever = db.as_retriever(search_kwargs={'k': 2}), \n",
    "                                                     combine_docs_chain_kwargs = {'prompt': prompt})\n",
    "\n",
    "    # qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "    #                                    chain_type='stuff',\n",
    "    #                                    retriever=db.as_retriever(search_kwargs={'k': 2}),\n",
    "    #                                    return_source_documents=True,\n",
    "    #                                    chain_type_kwargs={'prompt': prompt}\n",
    "    #                                    )\n",
    "    return qa_chain\n",
    "\n",
    "def load_llm():\n",
    "    # Load the locally downloaded model here\n",
    "    llm = HuggingFaceHub(repo_id = \"mistralai/Mistral-7B-Instruct-v0.1\", model_kwargs={'temperature': 0.5, \"max_length\": 512})\n",
    "    return llm\n",
    "\n",
    "def qa_bot(documents):\n",
    "    embeddings = HuggingFaceInstructEmbeddings(model_name='hkunlp/instructor-base')\n",
    "    db = FAISS.from_documents(documents, embeddings)\n",
    "    llm = load_llm()\n",
    "    qa_prompt = set_custom_prompt()\n",
    "    memory = ConversationBufferMemory(memory_key = 'chat_history', return_messages=True, output_key='answer')\n",
    "    qa = retrieval_qa_chain(llm, qa_prompt, db,memory)\n",
    "\n",
    "    return qa\n",
    "\n",
    "def final_result(query):\n",
    "    qa_result = qa_bot()\n",
    "    response = qa_result({'query': query})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shah.smit1/.local/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "qa_chain = qa_bot(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create a retreival chain that combines everything for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What does the paper conclude?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper concludes that abstractive summarization is a challenging NLP task, and that there is\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "        qa_chain({\"question\": query})['answer']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Gradio environment to run on web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2023/11/30 11:36:35 [W] [service.go:132] login to server failed: DialTcpByHttpProxy error, StatusCode [403]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def get_result(query):\n",
    "    print(query)\n",
    "    return qa_chain({\"query\": query})['result']\n",
    "\n",
    "demo = gr.Interface(fn=get_result, inputs=\"text\", outputs=\"text\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(show_api=False, share=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
