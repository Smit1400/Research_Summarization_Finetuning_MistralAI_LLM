text,summary,processed_text
Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond Ramesh Nallapati IBM Watson nallapatiusibmcomBowen Zhou IBM Watson zhouusibmcomCicero dos Santos IBM Watson ciceronsusibmcom aglar G ul ehre Universit de Montr al gulcehrciroumontrealcaBing Xiang IBM Watson bingxiausibmcom Abstract In this work we model abstractive text summarization using Attentional Encoder- Decoder Recurrent Neural Networks and show that they achieve state-of-the-art per- formance on two different corpora We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture such as modeling key-words capturing the hierarchy of sentence-to- word structure and emitting words that are rare or unseen at training time Our work shows that many of our proposed models contribute to further improvement in performance We also propose a new dataset consisting of multi-sentence sum- maries and establish performance bench- marks for further research Introduction Abstractive text summarization is the task of gen- erating a headline or a short summary consisting of a few sentences that captures the salient ideas of an article or a passage We use the adjective ab- stractive to denote a summary that is not a mere selection of a few existing passages or sentences extracted from the source but a compressed para- phrasing of the main contents of the document potentially using vocabulary unseen in the source document This task can also be naturally cast as map- ping an input sequence of words in a source doc- ument to a target sequence of words called sum- mary In the recent past deep-learning based mod- els that map an input sequence into another out- put sequence called sequence-to-sequence mod- els have been successful in many problems such as machine translation Bahdanau et al speech recognition Bahdanau et al and video captioning Venugopalan et al In the framework of sequence-to-sequence models a very relevant model to our task is the atten- tional Recurrent Neural Network RNN encoder- decoder model proposed in Bahdanau et al which has produced state-of-the-art per- formance in machine translation MT which is also a natural language task Despite the similarities abstractive summariza- tion is a very different problem from MT Unlike in MT the target summary is typically very short and does not depend very much on the length of the source document in summarization Addi- tionally a key challenge in summarization is to op- timally compress the original document in a lossy manner such that the key concepts in the original document are preserved whereas in MT the trans- lation is expected to be loss-less In translation there is a strong notion of almost one-to-one word- level alignment between source and target but in summarization it is less obvious We make the following main contributions in this work i We apply the off-the-shelf atten- tional encoder-decoder RNN that was originally developed for machine translation to summariza- tion and show that it already outperforms state- of-the-art systems on two different English cor- pora ii Motivated by concrete problems in sum- marization that are not suf ciently addressed by the machine translation based model we propose novel models and show that they provide addi- tional improvement in performance iii We pro- pose a new dataset for the task of abstractive sum- marization of a document into multiple sentences and establish benchmarks The rest of the paper is organized as follows In Section we describe each speci c problem in abstractive summarization that we aim to solve and present a novel model that addresses it Sec-arXiv06023v5 csCL Aug 2016tion contextualizes our models with respect to closely related work on the topic of abstractive text summarization We present the results of our ex- periments on three different data sets in Section We also present some qualitative analysis of the output from our models in Section before con- cluding the paper with remarks on our future di- rection in Section Models In this section we rst describe the basic encoder- decoder RNN that serves as our baseline and then propose several novel models for summarization each addressing a speci c weakness in the base- line Encoder-Decoder RNN with Attention and Large Vocabulary Trick Our baseline model corresponds to the neural ma- chine translation model used in Bahdanau et al The encoder consists of a bidirectional GRU-RNN Chung et al while the decoder consists of a uni-directional GRU-RNN with the same hidden-state size as that of the encoder and an attention mechanism over the source-hidden states and a soft-max layer over target vocabu- lary to generate words In the interest of space we refer the reader to the original paper for a de- tailed treatment of this model In addition to the basic model we also adapted to the summariza- tion problem the large vocabulary trick LVT described in Jean et al In our approach the decoder-vocabulary of each mini-batch is re- stricted to words in the source documents of that batch In addition the most frequent words in the target dictionary are added until the vocabulary reaches a xed size The aim of this technique is to reduce the size of the soft-max layer of the decoder which is the main computational bottle- neck In addition this technique also speeds up convergence by focusing the modeling effort only on the words that are essential to a given example This technique is particularly well suited to sum- marization since a large proportion of the words in the summary come from the source document in any case Capturing Keywords using Feature-rich Encoder In summarization one of the key challenges is to identify the key concepts and key entities in thedocument around which the story revolves In order to accomplish this goal we may need to go beyond the word-embeddings-based represen- tation of the input document and capture addi- tional linguistic features such as parts-of-speech tags named-entity tags and TF and IDF statis- tics of the words We therefore create additional look-up based embedding matrices for the vocab- ulary of each tag-type similar to the embeddings for words For continuous features such as TF and IDF we convert them into categorical values by discretizing them into a xed number of bins and use one-hot representations to indicate the bin number they fall into This allows us to map them into an embeddings matrix like any other tag-type Finally for each word in the source document we simply look-up its embeddings from all of its as- sociated tags and concatenate them into a single long vector as shown in Fig On the target side we continue to use only word-based embeddings as the representation Hidden State Input Layer Output Layer W POS NER TF IDFW POS NER TF IDFW POS NER TF IDFW POS NER TF IDFAttention mechanism ENCODERDECODER Figure Feature-rich-encoder We use one embedding vector each for POS NER tags and discretized TF and IDF values which are concatenated together with word-based em- beddings as input to the encoder Modeling RareUnseen Words using Switching Generator-Pointer Often-times in summarization the keywords or named-entities in a test document that are central to the summary may actually be unseen or rare with respect to training data Since the vocabulary of the decoder is xed at training time it cannot emit these unseen words Instead a most common way of handling these out-of-vocabulary OOV words is to emit an UNK token as a placeholder However this does not result in legible summaries In summarization an intuitive way to handle such OOV words is to simply point to their location in the source document instead We model this no-tion using our novel switching decoderpointer ar- chitecture which is graphically represented in Fig- ure In this model the decoder is equipped with a switch that decides between using the genera- tor or a pointer at every time-step If the switch is turned on the decoder produces a word from its target vocabulary in the normal fashion However if the switch is turned off the decoder instead gen- erates a pointer to one of the word-positions in the source The word at the pointer-location is then copied into the summary The switch is modeled as a sigmoid activation function over a linear layer based on the entire available context at each time- step as shown below Psi vsWs hhiWs eEoi Ws ccibs wherePsi is the probability of the switch turning on at the ithtime-step of the decoder hi is the hidden state Eoiis the embedding vec- tor of the emission from the previous time step ciis the attention-weighted context vector and Ws hWs eWs cbsandvsare the switch parame- ters We use attention distribution over word posi- tions in the document as the distribution to sample the pointer from Pa ijexpvaWa hhiWa eEoi Wa chd jba pi arg max jPa ijforj2f1N dg In the above equation piis the pointer value at ithword-position in the summary sampled from the attention distribution Pa iover the document word-positions j2f1N dg wherePa ijis the probability of the ithtime-step in the decoder pointing to the jthposition in the document and hd jis the encoders hidden state at position j At training time we provide the model with ex- plicit pointer information whenever the summary word does not exist in the target vocabulary When the OOV word in summary occurs in multiple doc- ument positions we break the tie in favor of its rst occurrence At training time we optimize the conditional log-likelihood shown below with ad- ditional regularization penalties logPyjx X igilogfPyijyixPsig gi logfPpijyixPsig where yandxare the summary and document words respectively giis an indicator function thatis set to whenever the word at position iin the summary is OOV with respect to the decoder vo- cabulary At test time the model decides automat- ically at each time-step whether to generate or to point based on the estimated switch probability Psi We simply use the arg max of the poste- rior probability of generation or pointing to gener- ate the best output at each time step The pointer mechanism may be more robust in handling rare words because it uses the encoders hidden-state representation of rare words to decide which word from the document to point to Since the hidden state depends on the entire context of the word the model is able to accurately point to unseen words although they do not appear in the target vocabulary Hidden State ENCODERDECODERInput Layer Output Layer G P G G G Figure Switching generatorpointer model When the switch shows G the traditional generator consisting of the softmax layer is used to produce a word and when it shows P the pointer network is activated to copy the word from one of the source document positions When the pointer is activated the embedding from the source is used as input for the next time-step as shown by the arrow from the encoder to the decoder at the bottom Capturing Hierarchical Document Structure with Hierarchical Attention In datasets where the source document is very long in addition to identifying the keywords in the document it is also important to identify the key sentences from which the summary can be drawn This model aims to capture this notion of two levels of importance using two bi-directional 1Even when the word does not exist in the source vocabu- lary the pointer model may still be able to identify the correct position of the word in the source since it takes into account the contextual representation of the corresponding UNK to- ken encoded by the RNN Once the position is known the corresponding token from the source document can be dis- played in the summary even when it is not part of the training vocabulary either on the source side or the target sideRNNs on the source side one at the word level and the other at the sentence level The attention mechanism operates at both levels simultaneously The word-level attention is further re-weighted by the corresponding sentence-level attention and re- normalized as shown below Paj Pa wjPa ssj PNd k1PawkPassk wherePa wjis the word-level attention weight at jthposition of the source document and sjis the ID of the sentence at jthword position Pa sl is the sentence-level attention weight for the lth sentence in the source Ndis the number of words in the source document and Pajis the re-scaled attention at the jthword position The re-scaled attention is then used to compute the attention- weighted context vector that goes as input to the hidden state of the decoder Further we also con- catenate additional positional embeddings to the hidden state of the sentence-level RNN to model positional importance of sentences in the docu- ment This architecture therefore models key sen- tences as well as keywords within those sentences jointly A graphical representation of this model is displayed in Figure Hidden State Word layer ENCODERDECODERInput Layer Output Layer Hidden State Sentence layer eos Sentence-level attention Word-level attention Figure Hierarchical encoder with hierarchical attention the attention weights at the word level represented by the dashed arrows are re-scaled by the corresponding sentence- level attention weights represented by the dotted arrows The dashed boxes at the bottom of the top layer RNN rep- resent sentence-level positional embeddings concatenated to the corresponding hidden states Related Work A vast majority of past work in summarization has been extractive which consists of identify- ing key sentences or passages in the source doc- ument and reproducing them as summary Neto etal Erkan and Radev Wong et al 2008a Filippova and Altun Colmenares et al Litvak and Last K Riedhammer and Hakkani-Tur Ricardo Ribeiro Humans on the other hand tend to paraphrase the original story in their own words As such hu- man summaries are abstractive in nature and sel- dom consist of reproduction of original sentences from the document The task of abstractive sum- marization has been standardized using the DUC- and DUC- competitions2The data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans The best performing system on the DUC- task called TOPIARY Zajic et al used a combination of linguistically motivated compression techniques and an unsu- pervised topic detection algorithm that appends keywords extracted from the article onto the com- pressed output Some of the other notable work in the task of abstractive summarization includes us- ing traditional phrase-table based machine transla- tion approaches Banko et al compression using weighted tree-transformation rules Cohn and Lapata and quasi-synchronous gram- mar approaches Woodsend et al With the emergence of deep learning as a viable alternative for many NLP tasks Collobert et al researchers have started considering this framework as an attractive fully data-driven alter- native to abstractive summarization In Rush et al the authors use convolutional models to encode the source and a context-sensitive at- tentional feed-forward neural network to generate the summary producing state-of-the-art results on Gigaword and DUC datasets In an extension to this work Chopra et al used a similar con- volutional model for the encoder but replaced the decoder with an RNN producing further improve- ment in performance on both datasets In another paper that is closely related to our work Hu et al introduce a large dataset for Chinese short text summarization They show promising results on their Chinese dataset using an encoder-decoder RNN but do not report exper- iments on English corpora In another very recent work Cheng and Lapata used RNN based encoder-decoder for ex- tractive summarization of documents This model is not directly comparable to ours since their 2httpducnistgovframework is extractive while ours and that of Rush et al Hu et al and Chopra et al is abstractive Our work starts with the same framework as Hu et al where we use RNNs for both source and target but we go beyond the standard architecture and propose novel models that ad- dress critical problems in summarization We also note that this work is an extended version of Nal- lapati et al In addition to performing more extensive experiments compared to that work we also propose a novel dataset for document summa- rization on which we establish benchmark num- bers too Below we analyze the similarities and differ- ences of our proposed models with related work on summarization Feature-rich encoder Sec Linguistic fea- tures such as POS tags and named-entities as well as TF and IDF information were used in many extractive approaches to summarization Wong et al 2008b but they are novel in the context of deep learning approaches for abstractive summa- rization to the best of our knowledge Switching generator-pointer model Sec This model combines extractive and abstractive approaches to summarization in a single end-to- end framework Rush et al also used a combination of extractive and abstractive ap- proaches but their extractive model is a sepa- rate log-linear classi er with handcrafted features Pointer networks Vinyals et al have also been used earlier for the problem of rare words in the context of machine translation Luong et al but the novel addition of switch in our model allows it to strike a balance between when to be faithful to the original source eg for named entities and OOV and when it is allowed to be cre- ative We believe such a process arguably mim- ics how human produces summaries For a more detailed treatment of this model and experiments on multiple tasks please refer to the parallel work published by some of the authors of this work Gulcehre et al Hierarchical attention model Sec Pre- viously proposed hierarchical encoder-decoder models use attention only at sentence-level Li et al The novelty of our approach lies in joint modeling of attention at both sentence and word levels where the word-level attention is further in- uenced by sentence-level attention thus captur-ing the notion of important sentences and impor- tant words within those sentences Concatenation of positional embeddings with the hidden state at sentence-level is also new Experiments and Results Gigaword Corpus In this series of experiments3 we used the anno- tated Gigaword corpus as described in Rush et al We used the scripts made available by the authors of this work4to preprocess the data which resulted in about 8M training examples The script also produces about 400K validation and test examples but we created a randomly sam- pled subset of examples each for validation and testing purposes on which we report our per- formance Further we also acquired the exact test sample used in Rush et al to make precise comparison of our models with theirs We also made small modi cations to the script to extract not only the tokenized words but also system- generated parts-of-speech and named-entity tags Training For all the models we discuss below we used dimensional word2vec vectors Mikolov et al trained on the same corpus to initial- ize the model embeddings but we allowed them to be updated during training The hidden state di- mension of the encoder and decoder was xed at in all our experiments When we used only the rst sentence of the document as the source as done in Rush et al the encoder vocabu- lary size was and that of the decoder stood at We used Adadelta Zeiler for training with an initial learning rate of We used a batch-size of and randomly shuf ed the training data at every epoch while sorting every batches according to their lengths to speed up training We did not use any dropout or regular- ization but applied gradient clipping We used early stopping based on the validation set and used the best model on the validation set to report all test performance numbers For all our models we employ the large-vocabulary trick where we re- strict the decoder vocabulary size to be- cause it cuts down the training time per epoch by nearly three times and helps this and all subse- 3We used Kyunghyun Chos code httpsgithub comkyunghyunchodl4mt-material as the start- ing point 4httpsgithubcomfacebookNAMAS 5Larger values improved performance only marginally but at the cost of much slower trainingquent models converge in only of the epochs needed for the model based on full vocab- ulary Decoding At decode-time we used beam search of size to generate the summary and limited the size of summary to a maximum of words since this is the maximum size we noticed in the sam- pled validation set We found that the average sys- tem summary length from all our models to agrees very closely with that of the ground truth on the validation set about words with- out any speci c tuning Computational costs We trained all our mod- els on a single Tesla K40 GPU Most models took about hours per epoch on an average except the hierarchical attention model which took hours per epoch All models typically converged within epochs using our early stopping criterion based on the validation cost The wall-clock training time until convergence therefore varies between days depending on the model Generating summaries at test time is reasonably fast with a throughput of about summaries per second on a single GPU using a batch size of Evaluation metrics Similar to Nallapati et al and Chopra et al we use the full length F1 variant of Rouge6to evaluate our sys- tem Although limited length recall was the pre- ferred metric for most previous work one of its disadvantages is choosing the length limit which varies from corpus to corpus making it dif cult for researchers to compare performances Full- length recall on the other hand does not impose a length restriction but unfairly favors longer sum- maries Full-length F1 solves this problem since it can penalize longer summaries while not impos- ing a speci c length restriction In addition we also report the percentage of tokens in the system summary that occur in the source which we call src copy rate in Table We describe all our experiments and results on the Gigaword corpus below words-lvt2k-1sent This is the baseline attentional encoder-decoder model with the large vocabulary trick This model is trained only on the rst sen- tence from the source document as done in Rush et al words-lvt2k-2sent This model is identical to the model above except for the fact that it is trained 6httpwwwberougecomPagesdefault aspxon the rst two sentences from the source On this corpus adding the additional sentence in the source does seem to aid performance as shown in Table We also tried adding more sentences but the performance dropped which is probably because the latter sentences in this corpus are not pertinent to the summary words-lvt2k-2sent-hieratt Since we used two sen- tences from source document we trained the hi- erarchical attention model proposed in Sec As shown in Table this model improves perfor- mance compared to its atter counterpart by learn- ing the relative importance of the rst two sen- tences automatically feats-lvt2k-2sent Here we still train on the rst two sentences but we exploit the parts-of-speech and named-entity tags in the annotated gigaword corpus as well as TF IDF values to augment the input embeddings on the source side as described in Sec In total our embedding vector grew from the original to and produced incre- mental gains compared to its counterpart words- lvt2k-2sent as shown in Table demonstrating the utility of syntax based features in this task feats-lvt2k-2sent-ptr This is the switching gener- atorpointer model described in Sec but in addition we also use feature-rich embeddings on the document side as in the above model Our ex- periments indicate that the new model is able to achieve the best performance on our test set by all three Rouge variants as shown in Table Comparison with state-of-the-art We com- pared the performance of our model words-lvt2k- 1sent with state-of-the-art models on the sample created by Rush et al as displayed in the bottom part of Table We also trained another system which we call words-lvt5k-1sent which has a larger LVT vocabulary size of 5k but also has much larger source and target vocabularies of 400K and 200K respectively The reason we did not evaluate our best vali- dation models here is that this test set consisted of only sentence from the source document and did not include NLP annotations which are needed in our best models The table shows that despite this fact our model outperforms the ABS model of Rush et al with statistical signi cance In addition our models exhibit better abstractive ability as shown by the src copy rate metric in the last column of the table Further our larger model words-lvt5k-1sent outperforms the state-of-the-artmodel of Chopra et al with statistically signi cant improvement on Rouge- We believe the bidirectional RNN we used to model the source captures richer contextual infor- mation of every word than the bag-of-embeddings representation used by Rush et al and Chopra et al in their convolutional atten- tional encoders which might explain our superior performance Further explicit modeling of im- portant information such as multiple source sen- tences word-level linguistic features using the switch mechanism to point to source words when needed and hierarchical attention solve speci c problems in summarization each boosting perfor- mance incrementally DUC Corpus The DUC corpus7comes in two parts the corpus consisting of document summary pairs and the corpus consisting of pairs Since these corpora are too small to train large neural networks on Rush et al trained their models on the Gigaword corpus but combined it with an additional log-linear extractive summa- rization model with handcrafted features that is trained on the DUC corpus They call the original neural attention model the ABS model and the combined model ABS Chopra et al also report the performance of their RAS- Elman model on this corpus and is the current state-of-the-art since it outperforms all previously published baselines including non-neural network based extractive and abstractive systems as mea- sured by the of cial DUC metric of recall at bytes In these experiments we use the same met- ric to evaluate our models too but we omit report- ing numbers from other systems in the interest of space In our work we simply run the models trained on Gigaword corpus as they are without tuning them on the DUC validation set The only change we made to the decoder is to suppress the model from emitting the end-of-summary tag and force it to emit exactly words for every summary since the of cial evaluation on this corpus is based on limited-length Rouge recall On this corpus too since we have only a single sentence from source and no NLP annotations we ran just the models words-lvt2k-1sent andwords-lvt5k-1sent The performance of this model on the test set 7httpducnistgovduc2004taskshtmlis compared with ABS and ABS models RAS- Elman from Chopra et al as well as TOP- IARY the top performing system on DUC- in Table We note our best model words-lvt5k-1sent outperforms RAS-Elman on two of the three vari- ants of Rouge while being competitive on Rouge- Model Rouge- Rouge- Rouge-L TOPIARY ABS ABS RAS-Elman words-lvt2k-1sent words-lvt5k-1sent Table Evaluation of our models using the limited-length Rouge Recall at bytes on DUC validation and test sets Our best model although trained exclusively on the Gi- gaword corpus consistently outperforms the ABS model which is tuned on the DUC- validation corpus in addi- tion to being trained on the Gigaword corpus CNNDaily Mail Corpus The existing abstractive text summarization cor- pora including Gigaword and DUC consist of only one sentence in each summary In this section we present a new corpus that comprises multi- sentence summaries To produce this corpus we modify an existing corpus that has been used for the task of passage-based question answering Hermann et al In this work the au- thors used the human generated abstractive sum- mary bullets from new-stories in CNN andDaily Mail websites as questions with one of the enti- ties hidden and stories as the corresponding pas- sages from which the system is expected to an- swer the ll-in-the-blank question The authors re- leased the scripts that crawl extract and generate pairs of passages and questions from these web- sites With a simple modi cation of the script we restored all the summary bullets of each story in the original order to obtain a multi-sentence sum- mary where each bullet is treated as a sentence In all this corpus has training pairs validation pairs and test pairs as de ned by their scripts The source documents in the train- ing set have words spanning sentences on an average while the summaries consist of words and sentences The unique character- istics of this dataset such as long documents and ordered multi-sentence summaries present inter- esting challenges and we hope will attract futureModel name Rouge- Rouge- Rouge-L Src copy rate Full length F1 on our internal test set 1words-lvt2k-1sent 2words-lvt2k-2sent 3words-lvt2k-2sent-hieratt 4feats-lvt2k-2sent 5feats-lvt2k-2sent-ptr Full length F1 on the test set used by Rush et al 6ABS Rush et al 7words-lvt2k-1sent 8RAS-Elman Chopra et al 9words-lvt5k-1sent Table Performance comparison of various models indicates statistical signi cance of the corresponding model with respect to the baseline model on its dataset as given by the con dence interval in the of cial Rouge script We report statistical signi cance only for the best performing models src copy rate for the reference data on our validation sample is Please refer to Section for explanation of notation Model Rouge- Rouge- Rouge-L words-lvt2k words-lvt2k-hieratt words-lvt2k-temp-att Table Performance of various models on CNNDaily Mail test set using full-length Rouge-F1 metric Bold faced numbers indicate best performing system researchers to build and test novel models on it The dataset is released in two versions one consisting of actual entity names and the other in which entity occurrences are replaced with document-speci c integer-ids beginning from Since the vocabulary size is smaller in the anonymized version we used it in all our exper- iments below We limited the source vocabulary size to 150K and the target vocabulary to 60K the source and target lengths to at most and words respectively We used dimensional word2vec embeddings trained on this dataset as input and we xed the model hidden state size at We also created explicit pointers in the train- ing data by matching only the anonymized entity- ids between source and target on similar lines as we did for the OOV words in Gigaword corpus Computational costs We used a single Tesla K- GPU to train our models on this dataset as well While the at models words-lvt2k and words- lvt2k-ptr took under hours per epoch the hier- archical attention model was very expensive con- suming nearly hours per epoch Convergence of all models is also slower on this dataset com- pared to Gigaword taking nearly epochs for all models Thus the wall-clock time for train- ing until convergence is about days for the at models but nearly days for the hierarchical at- tention model Decoding is also slower as well with a throughput of examples per second for at models and examples per second for the hierarchical attention model when run on a single GPU with a batch size of Evaluation We evaluated our models using the full-length Rouge F1 metric that we employed for the Gigaword corpus but with one notable differ- ence in both system and gold summaries we con- sidered each highlight to be a separate sentence Results Results from the basic attention encoder- decoder as well as the hierarchical attention model are displayed in Table Although this dataset is smaller and more complex than the Gigaword cor- pus it is interesting to note that the Rouge num- bers are in the same range However the hier- archical attention model described in Sec outperforms the baseline attentional decoder only marginally Upon visual inspection of the system output we noticed that on this dataset both these models pro- duced summaries that contain repetitive phrases or even repetitive sentences at times Since the summaries in this dataset involve multiple sen- tences it is likely that the decoder forgets what part of the document was used in producing earlier highlights To overcome this problem we used theTemporal Attention model of Sankaran et al that keeps track of past attentional weights of the decoder and expliticly discourages it from attending to the same parts of the document in fu- ture time steps The model works as shown by the 8On this dataset we used the pyrouge script https pypipythonorgpypipyrouge that al- lows evaluation of each sentence as a separate unit Addi- tional pre-processing involves assigning each highlight to its own a tag in the system and gold xml les that go as input to the Rouge evaluation script Similar evaluation was also done by Cheng and Lapata Source Document entity0 wanted lm director must be eager to shoot footage of golden lassos and invisible jets eos entity0 con rms that entity5 is leaving the upcoming entity9 movie the hollywood reporter rst broke the story eos entity5 was announced as director of the movie in november eos entity0 obtained a statement from entity13 that says given creative differences entity13 and entity5 have decided not to move forward with plans to develop and direct entity9 together eos entity0 and entity13 are both owned by entity16 eos the movie starring entity18 in the title role of the entity21 princess is still set for release on june eos it s the rst theatrical movie centering around the most popular female superhero eos entity18 will appear beforehand in entity25 v entity26 entity27 due out march eos in the meantime entity13 will need to nd someone new for the director s chair eos Ground truth Summary entity5 is no longer set to direct the rst entity9 theatrical movie eos entity5 left the project over creative differences eos movie is currently set for words-lvt2k entity0 con rms that entity5 is leaving the upcoming entity9 movie eos entity13 and entity5 have decided not to move forward with plans to develop eos entity0 con rms that entity5 is leaving the upcoming entity9 movie words-lvt2k-hieratt entity5 is leaving the upcoming entity9 movie eos the movie is still set for release on june eos entity5 is still set for release on june words-lvt2k-temp-att entity0 con rms that entity5 is leaving the upcoming entity9 movie eos the movie is the rst lm to around the most popular female actor eos entity18 will appear in entity25 due out march Table Comparison of gold truth summary with summaries from various systems Named entities and numbers are anonymized by the preprocessing script The eos tags represent the boundary between two highlights The temporal attention model words-lvt2k-temp-att solves the problem of repetitions in summary as exhibited by the models words-lvt2k andwords-lvt2k-hieratt by encouraging the attention model to focus on the uncovered portions of the documentfollowing simple equations tt1X k k t t t where tis the unnormalized attention-weights vector at the tthtime-step of the decoder In other words the temporal attention model down- weights the attention weights at the current time step if the past attention weights are high on the same part of the document Using this strategy the temporal attention model improves performance signi cantly over both the baseline model as well as the hierarchical attention model We have also noticed that there are fewer repetitions of summay highlights pro- duced by this model as shown in the example in Table These results although preliminary should serve as a good baseline for future researchers to compare their models against Qualitative Analysis Table presents a few high quality and poor qual- ity output on the validation set from feats-lvt2k- 2sent one of our best performing models Even when the model differs from the target summary its summaries tend to be very meaningful and rel- evant a phenomenon not captured by wordphrase matching evaluation metrics such as Rouge On the other hand the model sometimes misinter- prets the semantics of the text and generates a summary with a comical interpretation as shown in the poor quality examples in the table Clearly capturing the meaning of complex sentences re- mains a weakness of these models Our next example output presented in Figure displays the sample output from the switching generatorpointer model on the Gigaword corpus It is apparent from the examples that the model learns to use pointers very accurately not only for named entities but also for multi-word phrases Despite its accuracy the performance improve- ment of the overall model is not signi cant We believe the impact of this model may be more pro- nounced in other settings with a heavier tail distri- bution of rare words We intend to carry out more experiments with this model in the future On CNNDaily Mail data although our models are able to produce good quality multi-sentence summaries we notice that the same sentence orGood quality summary output S a man charged with the murder last year of a british back- packer confessed to the slaying on the night he was charged with her killing according to police evidence presented at a court hearing tuesday ian douglas previte is charged with murdering caroline stuttle of yorkshire england T man charged with british backpacker s death confessed to crime police of cer claims O man charged with murdering british backpacker con- fessed to murder S following are the leading scorers in the english premier league after saturday s matches alan shearer lrb- newcastle united rrb- james beattie T leading scorers in english premier league O english premier league leading scorers S volume of transactions at the nigerian stock exchange has continued its decline since last week a nse of cial said thursday the latest statistics showed that a total of million shares valued at million naira lrb- about million us dollars rrb- were traded on wednesday in deals T transactions dip at nigerian stock exchange O transactions at nigerian stock exchange down Poor quality summary output S broccoli and broccoli sprouts contain a chemical that kills the bacteria responsible for most stomach cancer say re- searchers con rming the dietary advice that moms have been handing out for years in laboratory tests the chemical unk killed helicobacter pylori a bacteria that causes stomach ulcers and often fatal stomach cancers T for release at unkmom was right broccoli is good for you say cancer researchers O broccoli sprouts contain deadly bacteria S norway delivered a diplomatic protest to russia on mon- day after three norwegian sheries research expeditions were barred from russian waters the norwegian research ships were to continue an annual program of charting sh resources shared by the two countries in the barents sea re- gion T norway protests russia barring sheries research ships O norway grants diplomatic protest to russia S jp morgan chase s ability to recover from a slew of recent losses rests largely in the hands of two men who are both looking to restore tarnished reputations and may be considered for the top job someday geoffrey unk now the co-head of jp morgan s investment bank left goldman sachs co more than a decade ago after executives say he lost out in a bid to lead that rm T executives to lead jp morgan chase on road to recov- ery O jp morgan chase may be considered for top job Table Examples of generated summaries from our best model on the validation set of Gigaword corpus S source document T target summary O system output Although we displayed equal number of good quality and poor quality summaries in the table the good ones are far more prevalent than the poor onesFigure Sample output from switching generatorpointer networks An arrow indicates that a pointer to the source po- sition was used to generate the corresponding summary word phrase often gets repeated in the summary We be- lieve models that incorporate intra-attention such as Cheng et al can x this problem by en- couraging the model to remember the words it has already produced in the past Conclusion In this work we apply the attentional encoder- decoder for the task of abstractive summarization with very promising results outperforming state- of-the-art results signi cantly on two different datasets Each of our proposed novel models ad- dresses a speci c problem in abstractive summa- rization yielding further improvement in perfor- mance We also propose a new dataset for multi- sentence summarization and establish benchmark numbers on it As part of our future work we plan to focus our efforts on this data and build more ro- bust models for summaries consisting of multiple sentences References Bahdanau et al Dzmitry Bahdanau Kyunghyun Cho and Yoshua Bengio Neural machine translation by jointly learning to align and translate CoRR abs Bahdanau et al Dzmitry Bahdanau Jan Chorowski Dmitriy Serdyuk Philemon Brakel and Yoshua Bengio End-to-end attention- based large vocabulary speech recognition CoRR abs Banko et al Michele Banko Vibhu O Mittal and Michael J Witbrock Headline genera- tion based on statistical translation In Proceedingsof the 38th Annual Meeting on Association for Com- putational Linguistics Cheng and Lapata2016 Jianpeng Cheng and Mirella Lapata Neural summarization by extracting sentences and words In Proceedings of the 54th An- nual Meeting of the Association for Computational Linguistics Cheng et al Jianpeng Cheng Li Dong and Mirella Lapata Long short-term memory-networks for machine reading CoRR abs Chopra et al Sumit Chopra Michael Auli and Alexander M Rush Abstractive sentence summarization with attentive recurrent neural net- works In HLT-NAACL Chung et al Junyoung Chung aglar G l ehre KyungHyun Cho and Yoshua Bengio Em- pirical evaluation of gated recurrent neural networks on sequence modeling CoRR abs Cohn and Lapata2008 Trevor Cohn and Mirella Lap- ata Sentence compression beyond word dele- tion In Proceedings of the 22Nd International Con- ference on Computational Linguistics Volume pages Collobert et al Ronan Collobert Jason Weston L on Bottou Michael Karlen Koray Kavukcuoglu and Pavel P Kuksa Natural lan- guage processing almost from scratch CoRR abs Colmenares et al Carlos A Colmenares Marina Litvak Amin Mantrach and Fabrizio Silvestri Heads Headline generation as sequence pre- diction using an abstract feature-rich space In Pro- ceedings of the Conference of the North Amer- ican Chapter of the Association for Computational Linguistics Human Language Technologies pages Erkan and Radev2004 G Erkan and D R Radev Lexrank Graph-based lexical centrality as salience in text summarization Journal of Arti cial Intelligence Research Filippova and Altun2013 Katja Filippova and Yasemin Altun Overcoming the lack of parallel data in sentence compression In Pro- ceedings of the Conference on Empirical Methods in Natural Language Processing pages Gulcehre et al Caglar Gulcehre Sungjin Ahn Ramesh Nallapati Bowen Zhou and Yoshua Ben- gio Pointing the unknown words In Pro- ceedings of the 54th Annual Meeting of the Associa- tion for Computational Linguistics Hermann et al Karl Moritz Hermann Tom s Kocisk Edward Grefenstette Lasse Espeholt Will Kay Mustafa Suleyman and Phil Blunsom Teaching machines to read and comprehend CoRR abs Hu et al Baotian Hu Qingcai Chen and Fangze Zhu Lcsts A large scale chinese short text summarization dataset In Proceedings of the Conference on Empirical Methods in Natural Lan- guage Processing pages Lisbon Portu- gal September Association for Computational Lin- guistics Jean et al S bastien Jean Kyunghyun Cho Roland Memisevic and Yoshua Bengio On using very large target vocabulary for neural ma- chine translation CoRR abs K Riedhammer and Hakkani-Tur2010 B Favre K Riedhammer and D Hakkani-Tur Long story short A S global unsupervised models for keyphrase based meeting summarization In Speech Communication pages Li et al Jiwei Li Minh-Thang Luong and Dan Jurafsky A hierarchical neural autoen- coder for paragraphs and documents CoRR abs Litvak and Last2008 M Litvak and M Last Graph-based keyword extraction for single- document summarization In Coling pages Luong et al Thang Luong Ilya Sutskever Quoc V Le Oriol Vinyals and Wojciech Zaremba Addressing the rare word problem in neural machine translation In Proceedings of the 53rd Annual Meeting of the Association for Computa- tional Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing pages Mikolov et al Tomas Mikolov Ilya Sutskever Kai Chen Greg Corrado and Jeffrey Dean Distributed representations of words and phrases and their compositionality CoRR abs Nallapati et al Ramesh Nallapati Bing Xiang and Bowen Zhou Sequence-to-sequence rnns for text summarization ICLR workshop abs Neto et al Joel Larocca Neto Alex Alves Fre- itas and Celso A A Kaestner Automatic text summarization using a machine learning ap- proach In Proceedings of the 16th Brazilian Sym- posium on Arti cial Intelligence Advances in Arti- cial Intelligence pages Ricardo Ribeiro2013 David Martins de Matos Jo co P Neto Anatole Gershman Jaime Carbonell Ri- cardo Ribeiro Lu s Marujo Self reinforce- ment for important passage retrieval In 36th inter- national ACM SIGIR conference on Research and development in information retrieval pages Rush et al Alexander M Rush Sumit Chopra and Jason Weston A neural attention model for abstractive sentence summarization CoRR abs Sankaran et al B Sankaran H Mi Y Al- Onaizan and A Ittycheriah Temporal Atten- tion Model for Neural Machine Translation ArXiv e-prints August Venugopalan et al Subhashini Venugopalan Marcus Rohrbach Jeff Donahue Raymond J Mooney Trevor Darrell and Kate Saenko Sequence to sequence video to text CoRR abs Vinyals et al O Vinyals M Fortunato and N Jaitly Pointer Networks ArXiv e-prints June Wong et al2008a Kam-Fai Wong Mingli Wu and Wenjie Li 2008a Extractive summarization using supervised and semi-supervised learning In Pro- ceedings of the 22Nd International Conference on Computational Linguistics Volume pages Wong et al2008b Kam-Fai Wong Mingli Wu and Wenjie Li 2008b Extractive summarization using supervised and semi-supervised learning In Pro- ceedings of the 22nd Annual Meeting of the Associa- tion for Computational Linguistics pages Woodsend et al Kristian Woodsend Yansong Feng and Mirella Lapata Title generation with quasi-synchronous grammar In Proceedings of the Conference on Empirical Methods in Natu- ral Language Processing EMNLP pages Stroudsburg PA USA Association for Com- putational Linguistics Zajic et al David Zajic Bonnie J Dorr and Richard Schwartz Bbnumd at duc- Topiary In Proceedings of the North American Chapter of the Association for Computational Lin- guistics Workshop on Document Understanding pages Zeiler2012 Matthew D Zeiler ADADELTA an adaptive learning rate method CoRR abs,Conclusion In this work we apply the attentional encoder- decoder for the task of abstractive summarization with very promising results outperforming state- of-the-art results signi cantly on two different datasets Each of our proposed novel models ad- dresses a speci c problem in abstractive summa- rization yielding further improvement in perfor- mance We also propose a new dataset for multi- sentence summarization and establish benchmark numbers on it As part of our future work we plan to focus our efforts on this data and build more ro- bust models for summaries consisting of multiple sentences ,abstractive text summarization using rnns beyond ramesh nallapati ibm watson nallapatiusibmcombowen zhou ibm watson zhouusibmcomcicero do santos ibm watson ciceronsusibmcom aglar g ul ehre universit de montr al gulcehrciroumontrealcabing xiang ibm watson bingxiausibmcom abstract work model abstractive text summarization using attentional encoder decoder recurrent neural network show achieve per formance two different corpus propose several novel model address critical problem summarization adequately modeled basic architecture modeling capturing hierarchy word structure emitting word rare unseen training time work show many proposed model contribute improvement performance also propose new dataset consisting sum mary establish performance bench mark research introduction abstractive text summarization task gen erating headline short summary consisting sentence capture salient idea article passage use adjective ab stractive denote summary mere selection existing passage sentence extracted source compressed para phrasing main content document potentially using vocabulary unseen source document task also naturally cast map ping input sequence word source doc ument target sequence word called sum mary recent past based mod el map input sequence another put sequence called mod el successful many problem machine translation bahdanau et al speech recognition bahdanau et al video captioning venugopalan et al framework model relevant model task atten tional recurrent neural network rnn encoder decoder model proposed bahdanau et al produced per formance machine translation mt also natural language task despite similarity abstractive summariza tion different problem mt unlike mt target summary typically short depend much length source document summarization addi tionally key challenge summarization op timally compress original document lossy manner key concept original document preserved whereas mt trans lation expected translation strong notion almost word level alignment source target summarization le obvious make following main contribution work apply atten tional rnn originally developed machine translation summariza tion show already outperforms state system two different english cor pora ii motivated concrete problem sum marization suf ciently addressed machine translation based model propose novel model show provide addi tional improvement performance iii pro pose new dataset task abstractive sum marization document multiple sentence establish benchmark rest paper organized follows section describe speci c problem abstractive summarization aim solve present novel model address cscl aug contextualizes model respect closely related work topic abstractive text summarization present result ex periments three different data set section also present qualitative analysis output model section con cluding paper remark future di rection section model section rst describe basic encoder decoder rnn serf baseline propose several novel model summarization addressing speci c weakness base line rnn attention large vocabulary trick baseline model corresponds neural chine translation model used bahdanau et al encoder consists bidirectional chung et al decoder consists size encoder attention mechanism state layer target vocabu lary generate word interest space refer reader original paper de tailed treatment model addition basic model also adapted summariza tion problem large vocabulary trick lvt described jean et al approach stricted word source document batch addition frequent word target dictionary added vocabulary reach xed size aim technique reduce size layer decoder main computational bottle neck addition technique also speed convergence focusing modeling effort word essential given example technique particularly well suited sum marization since large proportion word summary come source document case capturing keywords using encoder summarization one key challenge identify key concept key entity thedocument around story revolves order accomplish goal may need go beyond represen tation input document capture addi tional linguistic feature tag tag tf idf statis tic word therefore create additional based embedding matrix vocab ulary similar embeddings word continuous feature tf idf convert categorical value discretizing xed number bin use representation indicate bin number fall allows u map embeddings matrix like finally word source document simply embeddings sociated tag concatenate single long vector shown fig target side continue use embeddings representation hidden state input layer output layer w po ner tf idfw po ner tf idfw po ner tf idfw po ner tf idfattention mechanism encoderdecoder figure use one embedding vector po ner tag discretized tf idf value concatenated together em bedding input encoder modeling rareunseen word using switching summarization keywords test document central summary may actually unseen rare respect training data since vocabulary decoder xed training time emit unseen word instead common way handling oov word emit unk token placeholder however result legible summary summarization intuitive way handle oov word simply point location source document instead model using novel switching decoderpointer ar chitecture graphically represented fig ure model decoder equipped switch decides using genus tor pointer every switch turned decoder produce word target vocabulary normal fashion however switch turned decoder instead gen erates pointer one source word copied summary switch modeled sigmoid activation function linear layer based entire available context time step shown psi vsws hhiws eeoi w ccibs wherepsi probability switch turning decoder hi hidden state eoiis embedding vec tor emission previous time step ciis context vector w hws ew cbsandvsare switch parame ters use attention distribution word posi tions document distribution sample pointer pa ijexpvawa hhiwa eeoi wa chd jba pi arg max jpa dg equation piis pointer value summary sampled attention distribution pa iover document dg wherepa ijis probability decoder pointing jthposition document hd ji encoders hidden state position j training time provide model ex plicit pointer information whenever summary word exist target vocabulary oov word summary occurs multiple doc ument position break tie favor rst occurrence training time optimize conditional shown ad ditional regularization penalty logpyjx x igilogfpyijyixpsig gi logfppijyixpsig yandxare summary document word respectively giis indicator function thatis set whenever word position iin summary oov respect decoder vo cabulary test time model decides automat ically whether generate point based estimated switch probability psi simply use arg max poste rior probability generation pointing gener ate best output time step pointer mechanism may robust handling rare word us encoders representation rare word decide word document point since hidden state depends entire context word model able accurately point unseen word although appear target vocabulary hidden state encoderdecoderinput layer output layer g p g g g figure switching generatorpointer model switch show g traditional generator consisting softmax layer used produce word show p pointer network activated copy word one source document position pointer activated embedding source used input next shown arrow encoder decoder bottom capturing hierarchical document structure hierarchical attention datasets source document long addition identifying keywords document also important identify key sentence summary drawn model aim capture notion two level importance using two word exist source vocabu lary pointer model may still able identify correct position word source since take account contextual representation corresponding unk ken encoded rnn position known corresponding token source document dis played summary even part training vocabulary either source side target sidernns source side one word level sentence level attention mechanism operates level simultaneously attention corresponding attention normalized shown paj pa wjpa ssj pnd wherepa wjis attention weight jthposition source document sjis id sentence jthword position pa sl attention weight lth sentence source ndis number word source document pajis attention jthword position attention used compute attention weighted context vector go input hidden state decoder also con catenate additional positional embeddings hidden state rnn model positional importance sentence docu ment architecture therefore model key sen tences well keywords within sentence jointly graphical representation model displayed figure hidden state word layer encoderdecoderinput layer output layer hidden state sentence layer eos attention attention figure hierarchical encoder hierarchical attention attention weight word level represented dashed arrow corresponding sentence level attention weight represented dotted arrow dashed box bottom top layer rnn rep resent positional embeddings concatenated corresponding hidden state related work vast majority past work summarization extractive consists identify ing key sentence passage source doc ument reproducing summary neto etal erkan radev wong et al filippova altun colmenares et al litvak last k riedhammer ricardo ribeiro human hand tend paraphrase original story word hu man summary abstractive nature sel dom consist reproduction original sentence document task abstractive sum marization standardized using duc duc data task consists news story various topic multiple reference summary per story generated human best performing system duc task called topiary zajic et al used combination linguistically motivated compression technique unsu pervised topic detection algorithm appends keywords extracted article onto com pressed output notable work task abstractive summarization includes u ing traditional based machine transla tion approach banko et al compression using weighted rule cohn lapata gram mar approach woodsend et al emergence deep learning viable alternative many nlp task collobert et al researcher started considering framework attractive fully alter native abstractive summarization rush et al author use convolutional model encode source tentional neural network generate summary producing result gigaword duc datasets extension work chopra et al used similar con volutional model encoder replaced decoder rnn producing improve ment performance datasets another paper closely related work hu et al introduce large dataset chinese short text summarization show promising result chinese dataset using rnn report exper iments english corpus another recent work cheng lapata used rnn based ex tractive summarization document model directly comparable since extractive rush et al hu et al chopra et al abstractive work start framework hu et al use rnns source target go beyond standard architecture propose novel model ad dress critical problem summarization also note work extended version nal lapati et al addition performing extensive experiment compared work also propose novel dataset document summa rization establish benchmark num bers analyze similarity differ ences proposed model related work summarization encoder sec linguistic fea tures po tag well tf idf information used many extractive approach summarization wong et al novel context deep learning approach abstractive summa rization best knowledge switching model sec model combine extractive abstractive approach summarization single end framework rush et al also used combination extractive abstractive ap proaches extractive model sepa rate classi er handcrafted feature pointer network vinyals et al also used earlier problem rare word context machine translation luong et al novel addition switch model allows strike balance faithful original source eg named entity oov allowed cre ative believe process arguably mim ic human produce summary detailed treatment model experiment multiple task please refer parallel work published author work gulcehre et al hierarchical attention model sec pre viously proposed hierarchical model use attention li et al novelty approach lie joint modeling attention sentence word level attention uenced attention thus notion important sentence impor tant word within sentence concatenation positional embeddings hidden state also new experiment result gigaword corpus series used anno tated gigaword corpus described rush et al used script made available author preprocess data resulted training example script also produce validation test example created randomly sam pled subset example validation testing purpose report per formance also acquired exact test sample used rush et al make precise comparison model also made small modi cation script extract tokenized word also system generated tag training model discus used dimensional vector mikolov et al trained corpus initial ize model embeddings allowed updated training hidden state di mension encoder decoder xed experiment used rst sentence document source done rush et al encoder vocabu lary size decoder stood used adadelta zeiler training initial learning rate used randomly shuf ed training data every epoch sorting every batch according length speed training use dropout regular ization applied gradient clipping used early stopping based validation set used best model validation set report test performance number model employ trick strict decoder vocabulary size cause cut training time per epoch nearly three time help subse used kyunghyun chos code start ing point value improved performance marginally cost much slower trainingquent model converge epoch needed model based full vocab ulary decoding used beam search size generate summary limited size summary maximum word since maximum size noticed sam pled validation set found average sys tem summary length model agrees closely ground truth validation set word speci c tuning computational cost trained mod el single tesla gpu model took hour per epoch average except hierarchical attention model took hour per epoch model typically converged within epoch using early stopping criterion based validation cost training time convergence therefore varies day depending model generating summary test time reasonably fast throughput summary per second single gpu using batch size evaluation metric similar nallapati et al chopra et al use full length variant evaluate sys tem although limited length recall pre ferred metric previous work one disadvantage choosing length limit varies corpus corpus making dif cult researcher compare performance full length recall hand impose length restriction unfairly favor longer sum mary solves problem since penalize longer summary impos ing speci c length restriction addition also report percentage token system summary occur source call src copy rate table describe experiment result gigaword corpus baseline attentional model large vocabulary trick model trained rst sen tence source document done rush et al model identical model except fact trained aspxon rst two sentence source corpus adding additional sentence source seem aid performance shown table also tried adding sentence performance dropped probably latter sentence corpus pertinent summary since used two sen tences source document trained hi erarchical attention model proposed sec shown table model improves perfor mance compared atter counterpart learn ing relative importance rst two sen tences automatically still train rst two sentence exploit tag annotated gigaword corpus well tf idf value augment input embeddings source side described sec total embedding vector grew original produced incre mental gain compared counterpart word shown table demonstrating utility syntax based feature task switching gener atorpointer model described sec addition also use embeddings document side model ex periments indicate new model able achieve best performance test set three rouge variant shown table comparison com pared performance model model sample created rush et al displayed bottom part table also trained another system call larger lvt vocabulary size also much larger source target vocabulary respectively reason evaluate best vali dation model test set consisted sentence source document include nlp annotation needed best model table show despite fact model outperforms ab model rush et al statistical signi cance addition model exhibit better abstractive ability shown src copy rate metric last column table larger model outperforms chopra et al statistically signi cant improvement rouge believe bidirectional rnn used model source capture richer contextual infor mation every word representation used rush et al chopra et al convolutional atten tional encoders might explain superior performance explicit modeling im portant information multiple source sen tences linguistic feature using switch mechanism point source word needed hierarchical attention solve speci c problem summarization boosting perfor mance incrementally duc corpus duc two part corpus consisting document summary pair corpus consisting pair since corpus small train large neural network rush et al trained model gigaword corpus combined additional extractive summa rization model handcrafted feature trained duc corpus call original neural attention model ab model combined model ab chopra et al also report performance ra elman model corpus current since outperforms previously published baseline including network based extractive abstractive system mea sured cial duc metric recall byte experiment use met ric evaluate model omit report ing number system interest space work simply run model trained gigaword corpus without tuning duc validation set change made decoder suppress model emitting tag force emit exactly word every summary since cial evaluation corpus based rouge recall corpus since single sentence source nlp annotation ran model performance model test set compared ab ab model ra elman chopra et al well top iary top performing system duc table note best model outperforms two three vari ant rouge competitive rouge model rouge rouge topiary ab ab table evaluation model using rouge recall byte duc validation test set best model although trained exclusively gi gaword corpus consistently outperforms ab model tuned duc validation corpus addi tion trained gigaword corpus cnndaily mail corpus existing abstractive text summarization cor pora including gigaword duc consist one sentence summary section present new corpus comprises multi sentence summary produce corpus modify existing corpus used task question answering hermann et al work au thor used human generated abstractive sum mary bullet cnn anddaily mail website question one enti tie hidden story corresponding pa sage system expected swer question author leased script crawl extract generate pair passage question web site simple modi cation script restored summary bullet story original order obtain sum mary bullet treated sentence corpus training pair validation pair test pair de ned script source document train ing set word spanning sentence average summary consist word sentence unique character istics dataset long document ordered summary present inter esting challenge hope attract futuremodel name rouge rouge src copy rate full length internal test set full length test set used rush et al rush et al chopra et al table performance comparison various model indicates statistical signi cance corresponding model respect baseline model dataset given con dence interval cial rouge script report statistical signi cance best performing model src copy rate reference data validation sample please refer section explanation notation model rouge rouge table performance various model cnndaily mail test set using metric bold faced number indicate best performing system researcher build test novel model dataset released two version one consisting actual entity name entity occurrence replaced c beginning since vocabulary size smaller anonymized version used exper iments limited source vocabulary size target vocabulary source target length word respectively used dimensional embeddings trained dataset input xed model hidden state size also created explicit pointer train ing data matching anonymized entity id source target similar line oov word gigaword corpus computational cost used single tesla k gpu train model dataset well model word took hour per epoch hier archical attention model expensive con suming nearly hour per epoch convergence model also slower dataset com pared gigaword taking nearly epoch model thus time train ing convergence day model nearly day hierarchical tention model decoding also slower well throughput example per second model example per second hierarchical attention model run single gpu batch size evaluation evaluated model using rouge metric employed gigaword corpus one notable differ ence system gold summary con sidered highlight separate sentence result result basic attention encoder decoder well hierarchical attention model displayed table although dataset smaller complex gigaword cor pu interesting note rouge num bers range however hier archical attention model described sec outperforms baseline attentional decoder marginally upon visual inspection system output noticed dataset model pro duced summary contain repetitive phrase even repetitive sentence time since summary dataset involve multiple sen tences likely decoder forgets part document used producing earlier highlight overcome problem used thetemporal attention model sankaran et al keep track past attentional weight decoder expliticly discourages attending part document fu ture time step model work shown dataset used pyrouge script pypipythonorgpypipyrouge al low evaluation sentence separate unit addi tional involves assigning highlight tag system gold xml le go input rouge evaluation script similar evaluation also done cheng lapata source document wanted lm director must eager shoot footage golden lasso invisible jet eos con rms leaving upcoming movie hollywood reporter rst broke story eos announced director movie november eos obtained statement say given creative difference decided move forward plan develop direct together eos owned eos movie starring title role princess still set release june eos rst theatrical movie centering around popular female superhero eos appear beforehand v due march eos meantime need nd someone new director chair eos ground truth summary longer set direct rst theatrical movie eos left project creative difference eos movie currently set con rms leaving upcoming movie eos decided move forward plan develop eos con rms leaving upcoming movie leaving upcoming movie eos movie still set release june eos still set release june con rms leaving upcoming movie eos movie rst lm around popular female actor eos appear due march table comparison gold truth summary summary various system named entity number anonymized preprocessing script eos tag represent boundary two highlight temporal attention model solves problem repetition summary exhibited model encouraging attention model focus uncovered portion documentfollowing simple equation k k ti unnormalized vector decoder word temporal attention model weight attention weight current time step past attention weight high part document using strategy temporal attention model improves performance signi cantly baseline model well hierarchical attention model also noticed fewer repetition summay highlight pro duced model shown example table result although preliminary serve good baseline future researcher compare model qualitative analysis table present high quality poor qual ity output validation set one best performing model even model differs target summary summary tend meaningful rel evant phenomenon captured wordphrase matching evaluation metric rouge hand model sometimes misinter prets semantics text generates summary comical interpretation shown poor quality example table clearly capturing meaning complex sentence main weakness model next example output presented figure display sample output switching generatorpointer model gigaword corpus apparent example model learns use pointer accurately named entity also phrase despite accuracy performance improve ment overall model signi cant believe impact model may pro nounced setting heavier tail distri bution rare word intend carry experiment model future cnndaily mail data although model able produce good quality summary notice sentence orgood quality summary output man charged murder last year british back packer confessed slaying night charged killing according police evidence presented court hearing tuesday ian douglas previte charged murdering caroline stuttle yorkshire england man charged british backpacker death confessed crime police cer claim man charged murdering british backpacker con fessed murder following leading scorer english premier league saturday match alan shearer lrb newcastle united rrb james beattie leading scorer english premier league english premier league leading scorer volume transaction nigerian stock exchange continued decline since last week nse cial said thursday latest statistic showed total million share valued million naira lrb million u dollar rrb traded wednesday deal transaction dip nigerian stock exchange transaction nigerian stock exchange poor quality summary output broccoli broccoli sprout contain chemical kill bacteria responsible stomach cancer say searcher con rming dietary advice mom handing year laboratory test chemical unk killed helicobacter pylorus bacteria cause stomach ulcer often fatal stomach cancer release unkmom right broccoli good say cancer researcher broccoli sprout contain deadly bacteria norway delivered diplomatic protest russia mon day three norwegian sheries research expedition barred russian water norwegian research ship continue annual program charting sh resource shared two country barents sea gion norway protest russia barring sheries research ship norway grant diplomatic protest russia jp morgan chase ability recover slew recent loss rest largely hand two men looking restore tarnished reputation may considered top job someday geoffrey unk jp morgan investment bank left goldman sachs co decade ago executive say lost bid lead rm executive lead jp morgan chase road recov ery jp morgan chase may considered top job table example generated summary best model validation set gigaword corpus source document target summary system output although displayed equal number good quality poor quality summary table good one far prevalent poor onesfigure sample output switching generatorpointer network arrow indicates pointer source po sition used generate corresponding summary word phrase often get repeated summary lieve model incorporate cheng et al x problem en couraging model remember word already produced past conclusion work apply attentional encoder decoder task abstractive summarization promising result outperforming state result signi cantly two different datasets proposed novel model ad dress speci c problem abstractive summa rization yielding improvement perfor mance also propose new dataset multi sentence summarization establish benchmark number part future work plan focus effort data build ro bust model summary consisting multiple sentence reference bahdanau et al dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate corr ab bahdanau et al dzmitry bahdanau jan chorowski dmitriy serdyuk philemon brakel yoshua bengio attention based large vocabulary speech recognition corr ab banko et al michele banko vibhu mittal michael j witbrock headline genus tion based statistical translation proceedingsof annual meeting association com putational linguistics cheng jianpeng cheng mirella lapata neural summarization extracting sentence word proceeding nual meeting association computational linguistics cheng et al jianpeng cheng li dong mirella lapata long machine reading corr ab chopra et al sumit chopra michael auli alexander rush abstractive sentence summarization attentive recurrent neural net work chung et al junyoung chung aglar g l ehre kyunghyun cho yoshua bengio em pirical evaluation gated recurrent neural network sequence modeling corr ab cohn trevor cohn mirella lap ata sentence compression beyond word dele tion proceeding international con ference computational linguistics volume page collobert et al ronan collobert jason weston l bottou michael karlen koray kavukcuoglu pavel p kuksa natural lan guage processing almost scratch corr ab colmenares et al carlos colmenares marina litvak amin mantrach fabrizio silvestri head headline generation sequence pre diction using abstract space pro ceedings conference north amer ican chapter association computational linguistics human language technology page erkan g erkan r radev lexrank lexical centrality salience text summarization journal arti cial intelligence research filippova katja filippova yasemin altun overcoming lack parallel data sentence compression pro ceedings conference empirical method natural language processing page gulcehre et al caglar gulcehre sungjin ahn ramesh nallapati bowen zhou yoshua ben gio pointing unknown word pro ceedings annual meeting associa tion computational linguistics hermann et al karl moritz hermann tom kocisk edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machine read comprehend corr ab hu et al baotian hu qingcai chen fangze zhu lcsts large scale chinese short text summarization dataset proceeding conference empirical method natural lan guage processing page lisbon portu gal september association computational lin guistics jean et al bastien jean kyunghyun cho roland memisevic yoshua bengio using large target vocabulary neural chine translation corr ab k riedhammer b favre k riedhammer long story short global unsupervised model keyphrase based meeting summarization speech communication page li et al jiwei li luong dan jurafsky hierarchical neural autoen coder paragraph document corr ab litvak litvak last keyword extraction single document summarization coling page luong et al thang luong ilya sutskever quoc v le oriol vinyals wojciech zaremba addressing rare word problem neural machine translation proceeding annual meeting association computa tional linguistics international joint conference natural language processing asian federation natural language processing page mikolov et al tomas mikolov ilya sutskever kai chen greg corrado jeffrey dean distributed representation word phrase compositionality corr ab nallapati et al ramesh nallapati bing xiang bowen zhou rnns text summarization iclr workshop ab neto et al joel larocca neto alex alves fre itas celso kaestner automatic text summarization using machine learning ap proach proceeding brazilian sym posium arti cial intelligence advance arti cial intelligence page ricardo david martin de matos jo co p neto anatole gershman jaime carbonell ri cardo ribeiro lu marujo self reinforce ment important passage retrieval inter national acm sigir conference research development information retrieval page rush et al alexander rush sumit chopra jason weston neural attention model abstractive sentence summarization corr ab sankaran et al b sankaran h mi al onaizan ittycheriah temporal atten tion model neural machine translation arxiv august venugopalan et al subhashini venugopalan marcus rohrbach jeff donahue raymond j mooney trevor darrell kate saenko sequence sequence video text corr ab vinyals et al vinyals fortunato n jaitly pointer network arxiv june wong et wong mingli wu wenjie li extractive summarization using supervised learning pro ceedings international conference computational linguistics volume page wong et wong mingli wu wenjie li extractive summarization using supervised learning pro ceedings annual meeting associa tion computational linguistics page woodsend et al kristian woodsend yansong feng mirella lapata title generation grammar proceeding conference empirical method natu ral language processing emnlp page stroudsburg pa usa association com putational linguistics zajic et al david zajic bonnie j dorr richard schwartz bbnumd duc topiary proceeding north american chapter association computational lin guistics workshop document understanding page matthew zeiler adadelta adaptive learning rate method corr ab
